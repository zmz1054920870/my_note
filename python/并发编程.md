#  操作系统概念

https://www.cnblogs.com/Eva-J/p/7277026.html

https://www.cnblogs.com/Eva-J/articles/8253549.html#_label12

![](D:\笔记\python\线程模型.png)

**重点：**

- **如果我们实用的是multiprocessing 中的Process，在实用锁的时候，必须使用进程锁。同理实用Thread的时候必须实用线程锁，不然报错**
- **进程读取代码、数据、文件到自己的栈里面的时候，读取的是 `if __name__ == '__main__'上面的代码，文件、数据，而线程则不一样，由于线程是在同一个进程里面的，他们共享代码、数据、文件，所以线程读取的数据，就是整个py文件。`**

```python
import time
import random
# from multiprocessing import Process, Lock
from threading import Thread, Lock
# a = 11111111111111111111111111111111111111111

def ktv(i, sema):
    sema.acquire()
    print('%d走进ktv' % i)
    print(a)
    time.sleep(random.randint(1, 5)) #模拟唱歌事件
    print('%d走出ktv' % i)
    sema.release()

if __name__ == '__main__':
    a = 11111111111111111111111111111111111111111
    sema = Lock()
    for i in range(10):
        Thread(target=ktv, args=(i, sema)).start()
        
        
        
 ==========根据上面的第二点，线程贡献数据的代码，那么我们sema参数根本就不需要写到，参数里面去====
import time
import random
# from multiprocessing import Process, Lock
from threading import Thread, Lock
# a = 11111111111111111111111111111111111111111

def ktv(i):
    sema.acquire()
    print('%d走进ktv' % i)
    print(a)
    time.sleep(random.randint(1, 5)) #模拟唱歌事件
    print('%d走出ktv' % i)
    sema.release()

if __name__ == '__main__':
    sema = Lock()
    for i in range(10):
        Thread(target=ktv, args=(i, )).start()
```





#### Pool 、 ThreadPoolExecutor、ProcessPoolExecutor的区别

> - ​	🔺采用Pool.apply_async 进程池 的时候 主进程代码执行完毕以后，不会等待子进程执行完毕。而是直接结束
> - ​    ProcessPoolExecutor  主进程结束以后，会等待子进程，shutdown可以控制于主进程是否串行
> - ​    ThreadPoolExecutor   主线程结束以后，会等待子线程，shutdown可以控制于主进程是否串行
> - ​    说白了， Pool 的apply_async 生成的进程都是守护进程（当主进程代码执行完毕以后，守护线程跟着结束）



#### Pool 、 ThreadPoolExecutor、ProcessPoolExecutor的回调



##### ProcessPoolExecutor的回调

> - ​	ProcessPoolExecutor 回调是单独生成一个进程，该进程就是主进程,因为回调我们没有采用多进程直接实用的callback
> - ​    回调会阻塞多进程，比如我的进程池中有5个进程，那么这个5个进程+1个回调进程是一组， 当5个进程执行任务完毕后，且回调进程完成一次回调工作后，才重新拉取5个任务过来，这才是一轮（尼玛），这里有问题，现在的最新发现是，进程池每完成一个任务，就必须完成一个回调，才能继续工作，否则整个进程池阻塞
> -    最搞笑的是， 最后任务全部执行完毕后，才一点一点在回调进程中执行剩下的回调任务（5：1的执行嘛， 最后剩下一大堆回回调任务慢慢执行）
> -    这TM 其实跟不使用回调，采用result_list.append(res) 最后在串行执行回调的方式差距不大
> - ​    风险点：当 一组中的回调阻塞以后，整个组就阻塞了。GG思密达，因为单进程
> - ​    🔺不使用回调的时候，完成一个任务，释放回一个进程到进程池，然后被下一个任务占用

🔺. 建议ProcessPoolExecutor不使用回调，下面这个例子你可以看见,当 一组中的回调阻塞以后，整个组就阻塞了。GG思密达

```python
from multiprocessing import Manager
import time, random, os
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from multiprocessing import Pool

def func(q):
    time.sleep(random.randint(1,10))
    sign_field = random.randint(1, 100000)
    q.put(sign_field)
    print('创建标签:', sign_field, 'pid: %d' % os.getpid())
    return (q, sign_field)

def write_to_excel(result):
    random_num = random.randint(1,3)
    start_time = time.time()
    data = result.result()
    print('标签: %d 回调开始' % data[1], 'pid %d' % os.getpid())
    if random_num == 1:
        print('选中了100000')
        print(time.sleep(100000))
    print('标签: %d 回调结束' % random_num, '耗时: %d' % (time.time() - start_time))

if __name__ == '__main__':
    print('主进程', os.getpid())
    start_time = time.time()
    q = Manager().Queue()
    p_size = ProcessPoolExecutor(max_workers=5)
    for i in range(20):
        res = p_size.submit(func, q)
        res.add_done_callback(write_to_excel)
        print('用来感知回调是不是异步的: 答案,线程池的回调是异步的')
    p_size.shutdown()
    print('结束', time.time() - start_time)
    
 >>>
创建标签: 53019 pid: 13300
创建标签: 86882 pid: 15628
标签: 53019 回调开始 pid 21264
选中了100000
创建标签: 69907 pid: 11684
创建标签: 32788 pid: 13300
创建标签: 54978 pid: 17372
创建标签: 84486 pid: 13436
"""
GG思密达了，它必须等待100000秒以后，才会将这个五个进程归还到进程池中。。因为他们5个进程+1个回调 是一组，必须完成这一组以后，才将五个进程放回进程池中
"""

再看下面的例子
def func(q):
    time.sleep(random.randint(10, 10))
    sign_field = random.randint(1, 100000)
    q.put(sign_field)
    print('创建标签:', sign_field, 'pid: %d' % os.getpid())
    return (q, sign_field)

def write_to_excel(result):
    random_num = random.randint(1,3)
    start_time = time.time()
    data = result.result()
    print('标签: %d 回调开始' % data[1], 'pid %d' % os.getpid())
    # if random_num == 1:
    #     print('选中了100000')
    #     print(time.sleep(100000))
    time.sleep(10)
    print('标签: %d 回调结束' % random_num, '耗时: %d' % (time.time() - start_time))

if __name__ == '__main__':
    print('主进程', os.getpid())
    start_time = time.time()
    q = Manager().Queue()
    p_size = ProcessPoolExecutor(max_workers=5)
    for i in range(2):
        res = p_size.submit(func, q)
        res.add_done_callback(write_to_excel)
        print('用来感知回调是不是异步的: 答案,线程池的回调是异步的')
    p_size.shutdown()
    print('结束', time.time() - start_time)
    
>>>
主进程 21028
用来感知回调是不是异步的: 答案,线程池的回调是异步的
用来感知回调是不是异步的: 答案,线程池的回调是异步的
创建标签: 13249 pid: 19372
创建标签: 60910 pid: 12696
标签: 13249 回调开始 pid 21028
标签: 2 回调结束 耗时: 10
标签: 60910 回调开始 pid 21028
标签: 1 回调结束 耗时: 10
结束 30.26867914199829
🔺总共耗时 30秒，证明创建标签的2个任务是并行的，一共花费了10秒钟， 后面的回调，就是串行了。2个回调一共20秒，加起来30秒

再看这一个例子：更好说明。当回调阻塞以后，整个都阻塞了，我擦，不适合做回调
from concurrent.futures import ProcessPoolExecutor
import time
import random

def call(num):
    if num.result() == 1:
        time.sleep(1000)
    time.sleep(1)
    print(num.result())

def func(num):
    print('顾客 %d ' % num)
    time.sleep(random.randint(3, 10))
    return num



if __name__ == '__main__':
    p = ProcessPoolExecutor(max_workers=5)
    result_list = []
    start = time.time()
    for i in range(1, 20):
        res = p.submit(func, i)
        res.add_done_callback(call)
    print(1111111111111111111111111111111111)
    for i in result_list:
        print(i.get(), 'xxxxxxxxxxxxxxxxxxxxxx')
    p.shutdown()
    print(time.time()-start)
```









##### ThreadPoolExecutor

- ​    ThreadPoolExecutor回调就灵性很多了，一个生产任务加一个回调任务共占用一个线程，完成整个流程就把线程丢回池子里面去
- ​    🔺 ThreadPoolExecutor最后适合使用回调。回调和任务线程是一个，就算阻塞也只阻塞一个。
- ​     🔺不使用回调的时候，完成一个任务，释放回一个进程到进程池，然后被下一个任务占用。

```python
from concurrent.futures import ThreadPoolExecutor
import time
import random

def call(num):
    if num.result() == 1:
        time.sleep(1000)
    time.sleep(1)
    print(num.result())

def func(num):
    print('顾客 %d ' % num)
    time.sleep(random.randint(3, 10))
    return num



if __name__ == '__main__':
    p = ThreadPoolExecutor(max_workers=5)
    result_list = []
    start = time.time()
    for i in range(1, 20):
        res = p.submit(func, i)
        res.add_done_callback(call)
    print(1111111111111111111111111111111111)
    for i in result_list:
        print(i.get(), 'xxxxxxxxxxxxxxxxxxxxxx')
    p.shutdown()
    print(time.time()-start)
```





##### Pool进程池

> - ​	apply_async, 采用回调的时候，进程中完成一个任务就拉取下一个任务，回调数据相当于丢到了一个队列里面，回调函数慢慢取出来消耗。好像回调函数和主进程是一个（已经证实就是单进程的，在）。倒是任务处理非常块，但是最后的回调和主进程中获取结果就异常慢了。
> - ​    🔺res = p.apply_async(func, args=(q,), callback=(func2)), 这里回调的给func2的参数不需要get或者result，就是原始数据，直接使用，但是res是一个result对象，这个时候就需要res.get()来获取值了
> -  挺适合做回调的， 适合做哪些回调数据小，处理较快的场景

ProcessPoolExecutor ，进程池处理多个任务时，当有一个任务执行完成，，其他进程池任务继续未完成的任务，但是进程池要继续执行其他新任务，就必须等待，回调任务完成，如果回调阻塞，整个任务全部阻塞。感觉这就很坑了

```python
from multiprocessing import JoinableQueue, Process, Pool
# from concurrent.futures import ProcessPoolExecutor
import time
import random

def call(num):
    if num == 1:
        time.sleep(10000)
    time.sleep(5)
    print(num)

def func(num):
    print('顾客 %d ' % num)
    time.sleep(random.randint(3, 10))
    return num



if __name__ == '__main__':
    p = Pool(5)
    result_list = []
    start = time.time()
    for i in range(1, 20):
        res = p.apply_async(func, args=(i, ), callback=call)
        result_list.append(res)
    print(1111111111111111111111111111111111)
    for i in result_list:
        print(i.get(), 'xxxxxxxxxxxxxxxxxxxxxx')
    p.close()
    p.join()
    print(time.time()-start)
```



#### 🔺🔺进程池队列问题

`使用concurrent.futures中ProcessPoolExecutor或multiprocessing.Pool的apply_async创建进程池的时候，需要使用Queue来完成进程通信时，需要使用multiprocessing.Manager()中的Queue()，而不是multiprocessing.Queue()，否则会报错multiprocessing.Queue()不适合进程池，但是它适合线程池中使用`

```python
from multiprocessing import Manager
q = Manager().Queue()
```







## 关于while True

```
while True 相当耗费内存（1秒执行几千次几万次），所有我们要学会等待time.sleep(0.1)
```



# 进程的并发和并行

#### 并行

```
两者同时执行，比如赛跑，两个人都在不停的往前跑；（资源够用，比如三个线程，四核的cpu）
```



#### 并发

```
资源有限的情况下，两者交替轮流使用资源，比如一段路（单核cpu资源）同时只能过一个人，A走一段后，让给B，B走一段后继续给A，交替使用，目的是提高效率
```



#### 却别

```
并行是微观上，也就是在一个精确的时间片刻，有不同的程序在执行，这就要求必须有多个处理器
并发是从宏观上，在一个时间段上可以看出是同时执行，比如一个服务器同时处理多个session
```



#### 阻塞和非阻塞

```
🔺阻塞和非阻塞这两个概念与程序（线程）等待消息通知（无所谓同步或者异步）时的状态有关。也就是说阻塞和非阻塞主要是程序（线程）等待消息通知时的状态角度来说的
```



#### 进程的三状态图

![进程三状态图](D:\笔记\python\进程三状态图.png)



###### 例子

![例子](D:\笔记\python\进程运行例子.png)



#### 同步和异步

```
	所谓同步就是一个任务完成需要依赖另外一个任务， 只有等待被依赖的任务完成以后，依赖的任务才能算完成，这是一种可靠的任务序列。要么成功都成功，失败都失败，两个任务的状态可以保持一致  --- 串行的（我之前写的程序），你必须做完这件事，再去做另外一件（狭义理解）
	

	所谓异步是不需要等待被依赖的任务完成，只是同时被依赖的任务要完成什么工作，依赖的任务也立即执行，只要自己完成了整个任务就算完成了。至于被依赖的任务最终是否真正完成，依赖它的任务无法确定，所以它是不可靠的任务序列  --- 理解为，你写的两个函数在同时执行
```





#### 同步阻塞形式

```
效率最低， 比如你在银行办理业务， 这个时候你专心排队，什么别的事都不做 

解释
阻塞和非阻塞是从等待消息通知的角度来看的， 你自己在这里等待轮到你了（等待消息通知，是你在等待）

```



#### 异步阻塞形式

```
如果在银行等待办理业务采用的是异步的方式取等待消息被出发（通知），也就是领了一张小纸条，等待银行通知你，实际上你是可以出去干其他事情的， 但是银行要求你不能离开银行，不然到时候该你了你得不到通知，那么显然，你被阻塞在了这个等待操作上      和上面有很大的却别的

这个是告诉我们，异步操作是可以被阻塞住的，只不过他不是在处理消息时阻塞，而是等待消息通知时被阻塞
```





#### 异步非阻塞

```
效率高

你在银行办理业务，你领了一张小纸条，然后你坐在一个椅子上打电话，因为打电话是你（等待着）事情，而通知你则是柜台（消息出发机制）的事情，程序没有在两种不同操作中来回切换

再比如，这个人突然发觉自己烟瘾来了，需要出去抽烟，于是告诉大堂经理，排到我的时候麻烦到外面通知我一下，那么他就没有被阻塞在这个等待上面，自然这个就是异步+ 非阻塞的方式了
```



很多人会把同步和阻塞混淆，是因为很多时候同步操作会以阻塞的形式表现出来。同样的，很多人会把异步和非阻塞混淆，也因为异步操作一般都不会在真正的IO操作处被阻塞









## multiprocessing

```python
源码
   def __init__(self, group=None, target=None, name=None, args=(), kwargs={},
                 *, daemon=None):
        assert group is None, 'group argument must be None for now'
        count = next(_process_counter)
        self._identity = _current_process._identity + (count,)
        self._config = _current_process._config.copy()
        self._parent_pid = os.getpid()
        self._popen = None
        self._target = target
        self._args = tuple(args)
        self._kwargs = dict(kwargs)
        self._name = name or type(self).__name__ + '-' + \
                     ':'.join(str(i) for i in self._identity)
        if daemon is not None:
            self.daemon = daemon
        _dangling.add(self)

    def run(self):
        '''
        Method to be run in sub-process; can be overridden in sub-class
        方法在子进程中运行；可以在子类中重写 所以 创建子进程有2种形式
        '''
        if self._target:
            self._target(*self._args, **self._kwargs)
    
args 和 kwargs 是用来传递参数的    
from multiprocessing import Process

两种创建子进程的方式：
	1. p = Process(target=func, kwargs={'arg1': 10, 'arg2': 20})
    2. 继承Process,然后重写run()方法

第二种创建子进程例子如下：
class MyProcess(Process):

    def __init__(self, args1, args2):
        """
        注意 
        1.这里我们的参数不要和名不要和(group=None, target=None, name=None, args=(), kwargs={},*, daemon=None)同名，不然会覆盖了，导致报错
        2. 为什么super().__init__()的init中不加参数呢？ ，这是因为，上面的参数全部有默认参数。所以啊，这里也告诉我们一点，以后我们写父类的时候，也要这样搞。
        """
        super().__init__()
        
        self.args1 = args1
        self.args2 = args2

    def run(self):
        print(os.getpid())
        print(self.args1)
        print(self.args2)
if __name__ = '__main__':
    p1 = MyProcess(1, 2)
    p1.start()
    p2 = MyProcess(3,4)
    p2.start()

 两种方法没有优劣之分，要使用多进程类，就用上面这种， 要使用函数就使用函数多进程，也不完全这样

🔺在windows操作系统下面， 创建子进程必须在if __name__ = '__main__':下面， 在Linux和mac系统下面不需要


```



#### 进程的生命周期

```
🔺主进程代码会在代码执行完毕以后（代码执行完毕，不代表程序结束），等待子进程完毕之后  主进程才结束 （守护进程除外）
子进程不一定要依赖活着的主进程
```



#### os.getpid()

```
获取当前进程号		get process ID
```



#### os.getppid()

```
获取父进程的进程号  get parents process ID
```



#### join

是用于感知一个子进程的结束，例子如下：

```python
from multiprocessing import Process
import time


def func(arg1, arg2):
    print('*' * arg1)
    time.sleep(5)
    print('*' * arg2)


if __name__ == '__main__':
    p = Process(target=func, kwargs={'arg1': 10, 'arg2': 20})
    p.start()
    p.join()    #是感知一个子进程的结束，然后执行后面的
    print('------------: 运行完了')
   
```



###### 🔺现实中的例子  ---- 经典

```python
"""
同时写入10个文件，待10个文件都写完以后，再进行文件操作
	1. 必须等10个文件先写完以后在读取
	2. 不能同步进行（速度太慢）
"""

from multiprocessing import Process

def func(filename, content):
    with open(filename, 'w', encoding='utf8') as f:
        f.write(content)

if __name__ = '__main__':
    p_list = []
    for i in range(10):
    	p = Process(target=func, args=('file_%s' % i, '*' * i))
        p.start()
        p_list.append()
    """
    注意：for i in range(10) 也是主进程中的代码
    """
    [p.join() for p in p_list]

    print([i for i in os.walk(r'E":\python\day37')]) #获取创建的全部文件
        
```



#### 进程间数据隔离

###### 进程 与  进程之间 数据隔离（及执行顺序剖析）

```python
1. 他会把if __name__ == '__main__'外面的全部都读取进去，比如导入的模块，变量
2. if __name__ == '__main__'中的数据不会被读取
-----------------------------------------------------------------------------------------
from multiprocessing import Process
a = 1
def B():
    print(a)
print(1111111111111111111)
    
if __name__ == '__main__':
    a = 2
    p = Process(target=B)
    p.start()
    
>>> 1111111111111111111
>>>	1111111111111111111
>>>	1
-----------------------------------------------------------------------------------------

执行步骤解析：
from multiprocessing import Process
a = 1
def B():
    print(a)
print(1111111111111111111)  #执行到这里打印 1111111111111111111
if __name__ == '__main__':
    a = 2
    
开启进程，进程中代码如下:
from multiprocessing import Process
a = 1
def B():
    print(a)
print(1111111111111111111)  #执行到这里打印 1111111111111111111
if __name__ == '__main__':
    B()						#这里再去执行B函数，然后打印1
    
总结：所以最后的结果是
>>> 1111111111111111111
>>>	1111111111111111111
>>>	1

-----------------------------------------------------------------------------------------
    
    
    
    
进阶例子：
from multiprocessing import Process
a = 1
b = 2
def B(C):
    print(a)
    temp = C()

def C():
    print(b)


if __name__ == '__main__':
    a = 2
    b = 3
    p = Process(target=B, args=(C, ))
    p.start()
>>> 1
>>> 2
```



#### 守护进程

###### 是你起的子进程转换成的守护进程  子进程 -----> 守护进程

###### 主进程代码会在代码执行完毕以后，等待子进程执行完毕之后结束（这是正常的生命周期）  ---- 所以我们引入守护进程打破这个



```python

import time
from multiprocessing import Process

def func():

    while True:
        time.sleep(0.5)
        print('我还活着')


if __name__ == '__main__':
    p = Process(target=func， daemon=True)
    #daemon=True或者 p.daemon = True  设置子进程为守护进程，必须在p.start()前面申明
    p.start()

    i = 3
    while i > 0:
        time.sleep(2)
        print('我是socket server')
        i -= 1
```



###### 结论： 守护进程 会随着 主进程的代码执行完毕 而结束， 只关心主进程的代码是否执行完毕，不关心主进程结不结束。为什么这么说呢？请看下面的例子



```python
import time
from multiprocessing import Process

def func():

    while True:
        time.sleep(0.1)
        print('我还活着')

def fun2():
    print('func2 start....')
    time.sleep(8)
    print('func2 finished ...')

if __name__ == '__main__':
    p = Process(target=func, daemon=True)
    p.start()
    p2 = Process(target=fun2)
    p2.start()

    i = 2
    while i > 0:
        time.sleep(2)
        print('我是socket server')
        i -= 1
    print('主进程代码执行完毕')
    
  """
  这里守护进程在主进程代码执行完毕以后结束， 但是此时主进程还没有结束，它在等p2这个进程，当p2这个子进程执行完毕以后，主进程才会结束
  
  """
```







## 进程同步控制(锁/信号量/事件)



### 锁 -  multiprocessing.Lock


​    

```python
"""
场景： 买火车票
限制： 只剩下一张票
条件： 10个人同时去购票
要求： 只能一个人买到票
备注：D://自动化脚本-newpull//my_study_case//src//多进程//火车票， 文件中数据为{"ticket": 1}

"""
import json
import time
from multiprocessing import Process, Lock

def show_ticket():  #查询还有多少票
    with open('D://自动化脚本-newpull//my_study_case//src//多进程//火车票', 'r', encoding='UTF-8') as f:
        # a = f.read()
        # print(a)
        # f.seek(0)
        data = json.load(f)  # 将json格式转换成字典
        f.close()
        time.sleep(0.1)
        return data

def buy_ticket(func, i, lock):
    lock.acquire()
    data = func()
    num = data['ticket']
    print('余票还有%d' % num)
    time.sleep(0.1)
    if num > 0:
        time.sleep(5)  # 模拟网络延时，传入我的数据到服务器
        data['ticket'] -= 1  # 修改数据
        print('%d 买到票了' % i)
    else:
        print('%d 没有买到票' % i)
    time.sleep(0.1)
    with open('D://自动化脚本-newpull//my_study_case//src//多进程//火车票', 'w', encoding='utf-8') as f:
        json.dump(data, f)
    lock.release()

if __name__ == '__main__':
    lock = Lock()
    for i in range(10):
        p = Process(target=buy_ticket, args=(show_ticket, i, lock))
        p.start()
```



### 信号量 - multiprocessing.Semaphore





```python
"""
场景：ktv
限值：只有四个麦克风
条件：10个人去唱歌
要求：ktv包房只能容4个人进去唱歌，其他人在外面等着，等他们唱完

"""
import time
import random
from multiprocessing import Process, Semaphore

def ktv(i, sema):
    sema.acquire()
    print('%d走进ktv' % i)
    time.sleep(random.randint(1, 5)) #模拟唱歌事件
    print('%d走出ktv' % i)
    sema.release()

if __name__ == '__main__':
    sema = Semaphore(4)
    for i in range(10):
        Process(target=ktv, args=(i, sema)).start()
        
 备注： Semaphore是基于Lock实现的，只是其中加了一个计数器
```







### 事件 - multiprocessing.Event

备注：针对锁和信号量来说，他们都是acquire和release的过程，事件就不一样了

一个信号可以使所有的进程都进入阻塞状态，也可以控制所有的进程接触阻塞

一个事件被创建之后，默认是阻塞状态

🔺Event().set(), Event().clear(), Event().is_set(), Event().wait(),它们是可以在进程间通讯的，基于socket面向文件的通讯（不是内存共享）

```python
e = Event()         #创建一个事件
print(e.is_set())   #查看一个事件的状态，默认被设置成阻塞  >>> return Fasle 默认
print(123456)
print(e.set())      #将这个事件的状态改为True
e.wait()            #他的阻塞依赖于另外一个东西，就是e.wait, 他是依据e.is_set()的值来决定是否
                    #阻塞，False就阻塞， True就不阻塞
print(654321)

e.clear()           #将这个事件状态改为False

e.wait()
print(111111)

```



**红绿灯的例子**

```python
from multiprocessing import Process, Event
import time
import random


def taffic_light(e):
    while True:
        print('红灯')
        time.sleep(5)
        e.set()
        print('绿灯')
        e.clear()


def car(e, num):
    if not e.is_set():
        print('汽车 %d 红灯等待' % num)
        e.wait()
    print('汽车 %d 绿灯通过' % num)



if __name__ == '__main__':
    e = Event()
    p = Process(target=taffic_light, args=(e,))
    p.start()
    for i in range(10):
        time.sleep(random.randint(1, 3))
        c = Process(target=car, args=(e, i))
        c.start()

```





## 进程间通信(队列和管道)



#### 队列 - multiprocessing.Queue（进程间安全）

```
进程安全， 每次只能一个进程进入Queue取数据（就像锁一样），不存在多个进程操作同一个数据，同理JoinableQueue同样
```



```python
"""
队列遵循先进先出
"""
Queue().put(i)  >>>retrun None将i放入队列当中， 如果队列满了阻塞，等待数据被取走以后，继续放入
QUeue().get()	>>>return data返回队列中的数据, 如果队列空了阻塞，等待队列中有数据后，进行取出
Queue().full()  >>>return True or False,检查队列是否满了，在多进程的时候不稳定
Queue().empty()	>>>return True or False,检查队列是否为空，在多进程的时候不稳定


Queue([maxsize]) 
创建共享的进程队列。maxsize是队列中允许的最大项数。如果省略此参数，则无大小限制。底层队列使用管道和锁定实现。另外，还需要运行支持线程以便队列中的数据传输到底层管道中。 
Queue的实例q具有以下方法：

q.get( [ block [ ,timeout ] ] ) 
返回q中的一个项目。如果q为空，此方法将阻塞，直到队列中有项目可用为止。block用于控制阻塞行为，默认为True. 如果设置为False，将引发Queue.Empty异常（定义在Queue模块中）。timeout是可选超时时间，用在阻塞模式中。如果在制定的时间间隔内没有项目变为可用，将引发Queue.Empty异常。

q.get_nowait( ) 
同q.get(block=False)方法。


q.put(item [, block [,timeout ] ] ) 
将item放入队列。如果队列已满，此方法将阻塞至有空间可用为止。block控制阻塞行为，默认为True。如果设置为False，将引发Queue.Empty异常（定义在Queue库模块中）。timeout指定在阻塞模式中等待可用空间的时间长短。超时后将引发Queue.Full异常。

q.put_nowait()
同q.put(obj, block=False)

q.qsize() 
返回队列中目前项目的正确数量。此函数的结果并不可靠，因为在返回结果和在稍后程序中使用结果之间，队列中可能添加或删除了项目。在某些系统上，此方法可能引发NotImplementedError异常。


q.empty() 
如果调用此方法时 q为空，返回True。如果其他进程或线程正在往队列中添加项目，结果是不可靠的。也就是说，在返回和使用结果之间，队列中可能已经加入新的项目。

q.full() 
如果q已满，返回为True. 由于线程的存在，结果也可能是不可靠的（参考q.empty（）方法）。。
```



```python
from multiprocessing import Queue
q = Queue(5) #5 表示队列的大小
for i in range(6):
    q.put(i) #将i放到队列当中，但是队列大小只有5，当放入的数量大于队列数量的时候会阻塞
```



###### 例子1（待改进版本）

```
from multiprocessing import Queue, Process
import time
import random


def consumer(name, q):
    while True:
        food = q.get()
        if food is None:
            print('%s 获取到了一个空' % name)
            break
        print('\033[32;1m%s 消费一个 %s\033[0m' % (name, food))
        time.sleep(random.randint(1, 3))


def producer(name, food, q):
    for i in range(4):
        time.sleep(random.randint(1, 3))
        f = '\033[31;1m%s 生产的一个 %s\033[0m' % (name, food)
        print(f)
        q.put(f)


if __name__ == '__main__':
    q = Queue(20)
    p1 = Process(target=producer, args=('Tom', '包子', q))
    p2 = Process(target=producer, args=('John', '泔水', q))
    p1.start()
    p2.start()

    c1 = Process(target=consumer, args=('Alex', q))
    c2 = Process(target=consumer, args=('Laura', q))
    c1.start()
    c2.start()
    p1.join()
    p2.join()
    q.put(None)
    q.put(None)
```



#### multiprocessing.JoinableQueue（进程间安全的，不用担心同时操作多个进程同时操作一个数据）

```
JoinableQueue除了与Queue相同的方法之外，还具有2个特有的方法

1. q.task_done()
    消费者使用此方法发出信号，表示q.get()返回的项目已经被处理完成。如果调用此方法的次数大于从队列中删除的项目数量，将引发ValueError异常。
2. q.join()
    生产者使用此方法进行阻塞，直到队列中所有项目均被处理。阻塞将持续到为队列中的每个项目均调用q.task_done()方法为止。
```

```python
from multiprocessing import JoinableQueue

q = JoinableQueue(5)
q.put('包子')
q.put('泔水')
q.put('包子')
q.put('泔水')
q.put('包子')

for i in range(5):
    """
    JoinableQueue每次put的时候，就回去调用一次计数器,使得计数器中的slef._count加1(IPC，每一个调用JoinQueue的进程都会感知到这个计数), 每一次task_done的时候，也回去调用一次计数器,使得self._count减1
    
    现象：当range(4)的时候，q.join会阻塞，不会执行后面的print(’完成‘),当range(5)的时候，代码可以执行完毕
    
    原因: task_done()去调用计数器，每执行一次task_done，计数器中self._count减1, 当q.join感知到计数器中self._count==0的时候，解除阻塞
    
    为什么不用Queue.empty： 因为empty不准确
    
    注意:task_done就是去执行计数器减少操作的
    """
    q.get()
    a = q.task_done()

# q.put('包子')
# q.get()
# q.task_done()
q.join()
"""

q.join感觉感知计数器中的self._count， 当self._count == 0的时候，解除阻塞

"""
print('完成')
```



###### 例子2（例1的改进版）

```python
from multiprocessing import JoinableQueue, Process
import time
import random


def consumer(name, q):
    while True:
        food = q.get()
        if food is None:
            print('%s 获取到了一个空' % name)
            break
        print('\033[32;1m%s 消费一个 %s\033[0m' % (name, food))
        time.sleep(random.randint(1, 3))
        q.task_done()

def producer(name, food, q):
    for i in range(4):
        time.sleep(random.randint(1, 3))
        f = '\033[31;1m%s 生产的一个 %s\033[0m' % (name, food)
        print(f)
        q.put(f)
    q.join()

if __name__ == '__main__':
    q = JoinableQueue(20)
    p1 = Process(target=producer, args=('Tom', '包子', q))
    p2 = Process(target=producer, args=('John', '泔水', q))
    p1.start()
    p2.start()

    c1 = Process(target=consumer, args=('Alex', q), daemon=True)
    c2 = Process(target=consumer, args=('Laura', q), daemon=True)
    c1.start()
    c2.start()
    p1.join()
    p2.join()
```



#### 管道 - multiprocessing.Pipe（进程间不安全的，为什么讲管道呢？为了面试）

```
它是一个双向通信
```

![](D:\笔记\python\管道模型.png)

###### 例子

```python
from multiprocessing import Pipe, Process
import time
import os

def func1(conn):
    time.sleep(5)
    conn.send('吃了么')
    # print('func1 的进程号 %s' % os.getpid())
    print('func1 finshed ...')


def func2(conn):
    print('func2 waiting for recv...')
    data = conn.recv()
    print('func2 recv message : %s' % data)



if __name__ == '__main__':
    conn1, conn2 = Pipe()
    p1 = Process(target=func1, args=(conn1, ))
    p1.start()
    """
    IPC是基于文件的,所以p1在p2先执行，所以send在发送的时候，可能p2还没有建立
    但是由于是基于文件的，所以没事，  这个不一定准确，现在这样把
    """
    p2 = Process(target=func2, args=(conn2, ))
    p2.start()

>>>func2 waiting for recv...
>>>func1 finshed ...
>>>func2 recv message : 吃了么
```





###### 例子2（当数据接收完毕以后退出recv）

```python
from multiprocessing import Pipe, Process
import time
import os

def send1(conn1, conn2):
    conn2.close()   #不使用，直接先关闭了
    for i in range(20):
        conn1.send('吃了么')
        print('func1 finshed ...')
    conn1.close()

def recv2(conn1, conn2):
    conn1.close()   #不使用，直接先关闭了
    while True:
        try:
            print('func2 waiting for recv...')
            data = conn2.recv()
            print('func2 recv message : %s' % data)
        except EOFError:
            break
    conn2.close()



if __name__ == '__main__':
    conn1, conn2 = Pipe()
    p1 = Process(target=send1, args=(conn1, conn2))
    p1.start()
    # p1.join()
    p2 = Process(target=recv2, args=(conn1, conn2))
    p2.start()
    conn1.close()
    conn2.close()
    """
    提问：后面conn1.close() ,conn2.close()有何作用呢？
    回答：因为，只有管道的口都关闭以后，剩下的那个管道口在执行recv的时候，如果recv没有获取到消息的时候会报EOFError错误
    注意：这里其实有三个进程， 主进程， 子进程send1 和 recv2,所以都要关闭掉所有的口，只剩下1个
    
    """
```



## 进程间的数据共享(multiprocessing.Manager)



#### 共享 - multiprocessing.Manager



###### 玄学一： join

必须加join等待主进程必须等待子进程结束以后继续执行，或者time.sleep来等待子进程结束

```python
from multiprocessing import Manager, Process
import time
"""
主进程中不加p.join或者time.sleep()来等待子进程结束，是会报错的

"""

def func(dic):
    dic['count'] -= 1
    print(dic)

if __name__ == '__main__':
    m = Manager()
    dic = m.dict({'count': 100})
    p = Process(target=func, args=(dic, ))
    p.start()
    # p.join()
    # time.sleep()
    print('主进程: ', dic)
    
>>> FileNotFoundError: [WinError 2] 系统找不到指定的文件。
```



###### 玄学二： lock

数据不安全，存在多个进程同时处理同一个数据，所以要加锁



不加锁如下：

```python
from multiprocessing import Manager, Process


def func(dic):
    dic['count'] -= 1


if __name__ == '__main__':
    m = Manager()
    dic = m.dict({'count': 100})
    p_list = []
    for i in range(50):
        p = Process(target=func, args=(dic, ))
        p.start()
        p_list.append(p)
    [p.join() for p in p_list]
    print('主进程: ', dic)

 >>> 主进程:  {'count': 52}
```



加锁如下：

```python
from multiprocessing import Manager, Process, Lock


def func(dic, lock):
    lock.acquire()
    dic['count'] -= 1
    lock.release()

if __name__ == '__main__':
    l = Lock()
    m = Manager()
    dic = m.dict({'count': 100})
    p_list = []
    for i in range(50):
        p = Process(target=func, args=(dic, l))
        p.start()
        p_list.append(p)
    [p.join() for p in p_list]
    print('主进程: ', dic)
```



###### 爆炸性消息

![](D:\笔记\python\消息中间件.png)

以后工作中上面很多都不会用，扎心了，有跟成熟的消息中间件，而且Pipe很不安全，基本不用，Mannager也是



## 进程池(multiprocessing.Pool)--具有回调功能

###### 概要：一般我们启动守护进程或者一两个进程的时候我们使用Process， 多于5个我们就用进程池塘。。一台机器进程的数量最多控制在CPU个数 + 1（这是经验）

#### 参数说明：

```
Pool([numprocess  [,initializer [, initargs]]]):创建进程池!
```

```
1 numprocess:要创建的进程数，如果省略，将默认使用cpu_count()(cup的个数)的值
2 initializer：是每个工作进程启动时要执行的可调用对象，默认为None
3 initargs：是要传给initializer的参数组
```

```
1 p.apply(func [, args [, kwargs]]):在一个池工作进程中执行func(*args,**kwargs),然后返回结果。
2 '''需要强调的是：此操作并不会在所有池工作进程中并执行func函数。如果要通过不同参数并发地执行func函数，必须从不同线程调用p.apply()函数或者使用p.apply_async()'''
3 
4 p.apply_async(func [, args [, kwargs]]):在一个池工作进程中执行func(*args,**kwargs),然后返回结果。
5 '''此方法的结果是AsyncResult类的实例，callback是可调用对象，接收输入参数。当func的结果变为可用时，将理解传递给callback。callback禁止执行任何阻塞操作，否则将接收其他异步操作中的结果。'''
6    
7 p.close():关闭进程池，防止进一步操作。如果所有操作持续挂起，它们将在工作进程终止前完成
8 
9 P.jion():等待所有工作进程退出。此方法只能在close（）或teminate()之后调用
```

```
1 方法apply_async()和map_async（）的返回值是AsyncResul的实例obj。实例具有以下方法
2 obj.get():返回结果，如果有必要则等待结果到达。timeout是可选的。如果在指定时间内还没有到达，将引发一场。如果远程操作中引发了异常，它将在调用此方法时再次被引发。
3 obj.ready():如果调用完成，返回True
4 obj.successful():如果调用完成且没有引发异常，返回True，如果在结果就绪之前调用此方法，引发异常
5 obj.wait([timeout]):等待结果变为可用。
6 obj.terminate()：立即终止所有工作进程，同时不执行任何清理或结束任何挂起工作。如果p被垃圾回收，将自动调用此函数
```





#### map(self, func, iterable, chunksize=None): 方法（适合迭代任务处理）

```python
from multiprocessing import Process, Pool
def test(n):

    for i in range(100):
       print(n+1)
if __name__ == '__main__':
    pool = Pool(5)				#指定进程池大小  5个
    pool.map(test, range(100))	#任务数量100个

1. map自带join功能,apply也是
2. map的第二个参数必须是可迭代的
3. map的第二个参数是给test提供参数的，每次迭代得到的数据作为参数传递给func
4. pool.map(test, range(100)) 等价于，如下（实际上也不是一样,只是join和传参一样，对照看一下，好记）：

p_list = []
for i in range(100):
	p = Process(target=test, args=(i, ))
    p.start()
    p_list.append(p)
[p.join for p in p_list]
```



###### 例子(apply_async)

```python
from multiprocessing import Process, Pool
import os
import time

def func(n):
    print('start func %s' % n, os.getpid())
    time.sleep(1)
    print('end func %s' % n, os.getpid())
    # f = open('C://Users//zmz//Desktop//')


if __name__ == '__main__':
    p = Pool(5)
    for i in range(10):
        p.apply_async(func, args=(i, ))

    p.close()
    """
    这里的close不是关闭进程池，而是停止进程池继续接受任务
    """
    p.join()
    """
    这里的join不是感知子进程的结束，因为进程池中的进程是一直活着的，它只有
    处理工作状态和挂起状态， 这里是感知进程池中所有进程都处理完任务并处于挂起状态，
    由于：
        for i in range(10):
        	p.apply_async(func, args=(i, )) 已经将全部任务丢到进程池了，等待执行
    而且：
        丢进去以后，p.close() ,使进程池不再接收任务
    当：
        p.join()感知丢进去的任务是否全部执行完毕，当全部执行完毕以后，接触阻塞
    
    """
    print('完成10任务')
```



#### 返回结果（apply）

```python
def func(i):
    return i*i


if __name__ == '__main__':
    p = Pool(5)
    res = p.apply(func, args=(1, ))
    print(res)
    
 
>>> 1
```



#### 返回结果(apply_async) --- 异步执行，但同步返回结果

> - ​	🔺采用Pool.apply_async 进程池 的时候 主进程代码执行完毕以后，不会等待子进程执行完毕。而是直接结束
> - ​    ProcessPoolExecutor  主进程结束以后，会等待子进程，shutdown可以控制于主进程是否串行
> - ​    ThreadPoolExecutor   主线程结束以后，会等待子线程，shutdown可以控制于主进程是否串行



#### p.apply_async() --- 不会等待子进程结束，主进程结束就整体结束， 所有上面使用了p.close, p.join

```python
from multiprocessing import Pool
import time

def func():
    time.sleep(5)
    print(1)
    return time.time()


if __name__ == '__main__':
    p = Pool(5)
    result_list = []
    for t in range(20):
        res = p.apply_async(func, )
    #     result_list.append(res)
    # a = [res.get() for res in result_list]
    print('主进程代码结束')
    # print(a)
```





```python
from multiprocessing import Pool
import time

def func(i):
    time.sleep(5)
    return i*i


if __name__ == '__main__':
    p = Pool(5)
    for i in range(5):
        res = p.apply_async(func, args=(i, ))   #返回回调对象
        data = res.get()
        """
        获取回调结果，上面的p.apply_async(func, args=(i, )) 是异步的
        但是下面的res.get()会阻塞，等待回调结果，就变成了同步的了
        """
        print(data)
```



#### 返回结果(apply_async) --- 异步返回结果（进程池中执行完一个任务就返回结果）

```python
from multiprocessing import Pool
import time

def func(i):
    time.sleep(5)
    return i*i

if __name__ == '__main__':
    p = Pool(5)
    p_list = []
    for i in range(5):
        res = p.apply_async(func, args=(i, ))
        p_list.append(res)
    for p in p_list:
        print(p.get())
        
"""
执行完一个任务就返回结果

"""
```





#### 返回结果(map) ---执行是异步，但要等待全部任务结束返回结果(返回列表)

```python
from mulitprocessing import Pool
import time
def func(i):
    time.sleep(5)
    return i*i

if __name__ == '__main__':
    p = Pool(5)
    res = p.map(func, range(5))
    print(res)
    
>>>[0, 1, 4, 9, 16]
```



#### 回调函数

###### 特点：

1. 🔺回调函数是在主进程中执行的
2. 🔺 在主进程中回调，那就有一个问题了，那就是他会变成串行，而且还是按顺序执行的，我擦

```python
from multiprocessing import Pool
import os
def func1(i):
    print(os.getpid())
    return i*i

def func2(m):
    print(m, os.getpid())
    
if __name__ == '__main__':
    print('主进程', os.getpid())
    p = Pool(5)
    for i in range(10):
        p.apply_async(func1, args=(i, ), callback=func2)
    p.close()
    p.join()
```



2. 回调函数只能接受一个参数，但是如果返回的是多个值得时候，它会进行打包成一个元组

```python
def func1(i):
    return 1, 2

def func2(m):
    print(m)

if __name__ == '__main__':
    p = Pool(5)
    for i in range(10):
        p.apply_async(func1, args=(i, ), callback=func2)
    p.close()
    p.join()
```



#### 进程池 - multiprocessing.Pool





# 线程

https://www.cnblogs.com/Eva-J/articles/8306047.html

###### 概念：

​			进程是资源分配的最小单位，线程是CPU调度的最小单位



![](D:\笔记\python\线程模型.png)

　**线程与进程的区别**可以归纳为以下4点：

　　1）地址空间和其它资源（如打开文件）：进程间相互独立，同一进程的各线程间共享。某进程内的线程在其它进程不可见。

　　2）通信：[进程间通信](https://baike.baidu.com/item/进程间通信)[IPC](https://baike.baidu.com/item/IPC)，线程间可以直接读写进程数据段（如全局变量,全局打开的文件，代码（原始代码，代码里面产生的数据就互不影响了））来进行通信——需要[进程同步](https://baike.baidu.com/item/进程同步)和互斥手段的辅助，以保证数据的一致性。

　　3）调度和切换：线程上下文切换比进程上下文切换要快得多。

　　4）在多线程操作系统中，进程不是一个可执行的实体。

　　[*通过漫画了解线程进城](http://www.ruanyifeng.com/blog/2013/04/processes_and_threads.html)





#  拓展

#### window实现多进程的痛点

```
在window上面多进程必须在if __name__ == '__main__'中执行
```



#### 解决

```python
import os.path
import runpy

path = os.path.dirname(__file__)
file = os.path.join(path, 'tudou.py')
print(file)
a = runpy.run_path(file, run_name='__main__')
```

使用runpy就可以在其他模块中执行它，但是它会等待file执行完毕以后继续执行



#### 同个进程中的多个线程间数据共享（同样，主线代码执行完毕后会等待子线程结束后一起结束，同样有join方法）

```python
from threading import Thread
import os
import time

def func(a, b):
    time.sleep(1)
    n = a + b
    print(n, os.getpid())

t_list = []
for i in range(10):
    t = Thread(target=func, args=(i, 5))
    t.start()
    t_list.append(t)

for t in t_list:
    t.join()

print('主进程', os.getpid())
```





#### 阶段总结

```
1. 进程是最小的 内存分配单位
2. 线程是操作系统调度的最小单位
3. 线程是被CPU真正执行的
4. 进程内至少含有一个线程
5. 进程中可以开启多个线程
	5.1 开启一个线程所需的时间要远远小于开启一个进程的时间
	5.2 多个线程内部有自己的数据栈，数据不共享
	5.3 全局变量在多个线程之间是共享的
```



#### 概念讲解1（同一个进程中的多个线程，可以同时在不同的CUP上面运行，却别与前面的进程，因为前面我们学的进程只有一个主线程）

![](D:\笔记\python\概念.png)



###### 同一个进程中的多个线程，可以同时在不同的CUP上面运行的问题，但是会出现GIL（全局解释锁）

![](D:\笔记\python\GIL.png)



#### 概念2（CPU只做运算和调度，这里的调度就是，比如你要写东西，它就调度I/O系统来处理你的这个东西）



假如我的电脑（1个CPU8核， 16线程）  -- 就是我当前的电脑配置

由于GIL的存在， 那么python的多线程并发是不是没有意义了呢？  不是的，当我们在做计算操作的时候，如（1+1）， 这个时候处于对数据保护，会触发全局解释锁（GIL：当cpu在计算1+1的时候，会锁住全部cpu来操作这块1+1的内存空间，当它计算完成以后，再将数据写入后，把锁还回去。这个时候其他的线程才会被cpu执行的，导致无法实现高并发）， 但是我们日常生活和开发中， 高cpu的计算类很少，而高I/O的很多，高I/O是不会使用cpu的



#### 总结上面两个概念：

之所以说这么多，就是要告诉你们， 当我们在使用多线程处理计算或者高CPU操作的时候，他是存在GIL锁的



#### 例子

```python
from threading import Thread
import time

def func():
    global n
    temp = 10
    time.sleep(0.2)
    n =temp -1
n = 10
t_list = []
for i in range(10):
    t = Thread(target=func)
    t.start()
    t_list.append(t)

for t in t_list: t.join()
print(n)
>>> 9 

```

![](D:\笔记\python\GIL_例子.png)



###### 疑问

为什么会是9呢？ ，不是说存在GIL,答案应该是0啊？

###### python执行机制：

首先python的代码执行是由python虚拟机（又名解释器主循环）进行控制的。python在设计时是这样考虑的，在主循环中同时只能有一个控制线程在执行。就像单核CPU系统中的多进程一样。内存中有许多程序，但是在任意时刻只能有一个程序在运行。同理，尽管python解释器中可以运行多个线程，但是在任意给定时刻只有一个线程会被解释器执行。

###### 上面例子解释：

由于存在GIL锁的机制， 为了保护进程中数据n的安全（同一个进程中多个线程数据共享）， 线程被python解释器分配给CPU执行的时候，其他线程被GIL了，但是由于，time.sleep, 切换了时间片，第一线程设置会睡眠状态，并解开GIL锁， 第二个线程被CPU执行的时候，由于第一个线程只是拿到了数据n（拿到对应线程的栈当中，此时的数据是不共享的）但是没有进行数据计算，此时的n的数值还没有变，由于数据共享，第二个线程拿到的n还是10，放到他们的栈当中，同理后面的几个线程拿到的n都是10。 当时间片轮到第一个线程的时候，由于它的栈里面已经有n=10了，然后再进行计算，然后写入内存当中，后面几个线程同理， 写回去的都是9，所有最后的结果是9.。。。。。。

###### 说明了什么问题呢？

有GIL 数据不一定安全



## 线程锁---死锁

前置条件：同时拿到面条和叉子才能吃面，面条和叉子上面都有一把锁

现象：3个人去吃面，第一个人抢到了叉子，第二个人抢到了面条，由于叉子和面条都上锁了，就一直等待另外一个人放回叉子或者面条才能完成吃面任务，但是由于他们的任务（吃面）都没有完成，导致他们一直等待，造成了死锁

![](D:\笔记\python\科学家吃面.png)



###### 递归锁：  RLock   用于解决死锁现象

```python
from threading import RLock
rlock = RLock()
rlock.acquire()
rlock.acquire()
rlock.acquire()
rlock.acquire()
rlock.acquire()
print(123)
>>> 123

他是一串钥匙， 前面写了多少个acquire,后面就要release多少下，这样才可以完全解锁，然后是将整个钥匙串还回去
```



###### 什么时候会出现死锁现象呢？

当同一个线程当中存在两把及其以上的锁的时候？这个时候就会出现死锁现象，我们怎么解决呢？采用递归锁，递归锁，在同一个线程中可以多次acquire





#### Event

```python
"""
启两个线程
第一个线程：连接数据库
    #等待一个信号 告诉我 ， 我们之间的网络是通的
    #连接数据库

第二个进程：检查与数据库之间网络是否连通
    #time.sleep(0~3)  超过三秒任务是不通的
    #将事件状态设置为True
"""

from threading import Thread, Event
import time
import random

def check_web(e):
    while True:
        t1 = time.time()
        time.sleep(random.randint(0, 5))
        t2 = time.time()
        total = t2 - t1
        print('连接时间:', total)
        if total < 4:
            e.set()
        else:
            e.clear()

def connect_db(e):
    if e.wait(timeout=4):
        print('数据库网络正常')
        e.clear()
        return {'code': 200, 'msg': '数据库连接状态正常'}
    print('数据库网路异常')
    return {'code': 403, 'msg': '数据连接状态异常'}

e = Event()
check = Thread(target=check_web, args=(e, ))
check.start()
conn_db = Thread(target=connect_db, args=(e, ))
conn_db.start()


改进版：
"""
启两个线程
第一个线程：连接数据库
    #等待一个信号 告诉我 ， 我们之间的网络是通的
    #连接数据库

第二个进程：检查与数据库之间网络是否连通
    #time.sleep(0~3)  超过三秒任务是不通的
    #将事件状态设置为True
    #给三次尝试的机会
"""

from threading import Thread, Event
import time
import random

def check_web(e):
    while True:
        t1 = time.time()
        time.sleep(random.randint(0, 5))
        t2 = time.time()
        total = t2 - t1
        print('连接时间:', total)
        if total < 3:
            e.set()
        else:
            e.clear()

def connect_db(e):
    count = 0
    while count < 3:
        if e.wait(timeout=3):
            print('数据库网络正常')
            e.clear()
            return {'code': 200, 'msg': '数据库连接状态正常'}
        print('数据库网路异常, 尝试再次连接')
        count += 1
    return {'code': 403, 'msg': '数据连接状态异常'}

e = Event()
check = Thread(target=check_web, args=(e, ))
check.start()
conn_db = Thread(target=connect_db, args=(e, ))
conn_db.start()
```







#### Condition  --- 条件 （基本没有应用场景，基本用于面试场景）

```python
con = Condition() #实例化

 方法：
 con.notify(int：)	造钥匙几个把钥匙
 con.acquire()		接受钥匙通知（是否造好， 必须要接受通知，不然你不知道钥匙造没造好）
 con.wait()			等待开锁，有个timeout参数， 当超过timeout参数以后，破门而入
 con.release()		销毁钥匙
 
 """
 1. notify和wait必须在acquire和release之间
 2. 这里的钥匙是一次性的，使用完即销毁
 """
```



###### 列子

```python
from threading import Thread, Condition

def func(con, i):
    con.acquire()
    con.wait(5)
    print('在第%s个循环里' % i)
    con.release()

con = Condition()
for i in range(10):
    Thread(target=func, args=(con, i)).start()
while True:
    num = int(input('>>>'))
    con.acquire()
    con.notify(num)
    con.release()
```





#### Timer  --- 定时器

```python
from threading import Timer
def func():
    print('BOM ~~~~~~~~~ ')

Timer(2, func).start()
print('还有2秒钟爆炸')

"""
不是立即开启一个进程而是延时一段时间后执行
"""
```



#### queue --- 线程队列

```python
import queue
"""
先进先出队列
"""
q = queue.Queue(5)
q.put()
q.get()
q.get_nowait()		#没有数据以后，触发queue.Empty
q.put_nowait()		#数据满了以后，触发queue.Full
-----------------------------------------------------------------------------------------
q = queue.LifoQueue(5)
"""
先进后出队列
lifo 是一个英文，译为：先进后出 ---- 真TMD直接
LifoQueue:就跟栈一样先进后出，跟列表的pop一样
"""
q.put()
q.get()
q.get_nowait()		#没有数据以后，触发queue.Empty
q.put_nowait()		#数据满了以后，触发queue.Full


-----------------------------------------------------------------------------------------

q = queue.Priority(5)
"""
优先级队列
"""
q.put((优先级(int), data))    			int的数字越小，优先级越高
q.get()		>>>返回一个元组(1, 'a')	优先级越高先出，同一优先级，ASCII码靠前的先出	


```





#### 进程线程池 ---- concurrent.futures（由于有GIL锁的原因， threading没有线程池，concurrent.futures的线程池也有点问题）

```python
#1 介绍
    concurrent.futures模块提供了高度封装的异步调用接口
    ThreadPoolExecutor：线程池，提供异步调用
    ProcessPoolExecutor: 进程池，提供异步调用
    Both implement the same interface, which is defined by the abstract Executor class.

#2 基本方法
    #submit(fn, *args, **kwargs)
    异步提交任务

    #map(func, *iterables, timeout=None, chunksize=1) 
    取代for循环submit的操作

    #shutdown(wait=True) 
    相当于进程池的pool.close()+pool.join()操作
    wait=True，等待池内所有任务执行完毕回收完资源后才继续
    wait=False，立即返回，并不会等待池内的任务执行完毕
    但不管wait参数为何值，整个程序都会等到所有任务执行完毕
    submit和map必须在shutdown之前

    #result(timeout=None)
    取得结果

    #add_done_callback(fn)
    回调函数
    # done()判断某一个线程是否完成# cancle()取消某个任务
```



#### 线程池回调的例子

```python
from concurrent.futures import ThreadPoolExecutor

def func(data):
    print(data._result) # 获取结果

def get(i):
    return i

if __name__ == '__main__':
    t_pool = ThreadPoolExecutor(max_workers=2)
    res_list = []
    for i in range(10):
        res = t_pool.submit(get, i)	# 这个才是创建线程，创建完就走，让线程自己玩， 其他的代码都是主线程中的，都是串行的
        res_list.append(res)		# 还是要这样写，先把全部任务丢进去，不能把res.add_done_callback(func)写在这一行，这个是主线程中的代码，它会等待执行结束才继续走。这就TM 的没意义了啊
       
    res.add_done_callback(func)

    # t_pool.shutdown()
    print('结束')
```

get
11111111111111111
func
22222222222222222
get
11111111111111111

###### 线程池例子(ThreadPoolExecutor) --- 等待任务处理在执行主进程

```python
from concurrent.futures import ThreadPoolExecutor
import time

def func(n):
   time.sleep(2)
   # print(n)
   return n*n

tpool = ThreadPoolExecutor(max_workers=5)
for i in range(20):
    t = tpool.submit(func, i)
tpool.shutdown(wait=True)   #不写wait参数，默认是True
print('完成')

```



###### 线程池例子(ThreadPoolExecutor) --- 返回结果，返回的结果一定是按照顺序的

> - ​	t = tpool.submit(func, i) 传参一定要写成这个样子，不能写成t = tpool.submit(func, args=(i,)),这里面有问题，我日

```python
from concurrent.futures import ThreadPoolExecutor
import time


def func(n):
   time.sleep(2)
   # print(n)
   return n*n

tpool = ThreadPoolExecutor(max_workers=5)
t_result = []
for i in range(20):
    t = tpool.submit(func, i)
    t_result.append(t)
# tpool.shutdown(wait=True)   #这里不需要shutdown了，因为q.result的时候会等待结果
for q in t_result:
    print(q.result())			#返回的结果一定是按照顺序的，因为t_result.append(t)是按顺序放入的，所有结果结果也是按顺序接受，如果前面的还没有完成他会阻塞等待，那么你后面的已经返回了结果
```



#### 线程池队列思路

```python
import time, random, os
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from multiprocessing import Pool


def func(q):
    time.sleep(5)
    sign_field = random.randint(1, 100000)
    q.put(sign_field)
    print('标签:', sign_field)
    return (q, sign_field)

def write_to_excel(result):
    data = result.result()
    print('标签: %d 回调开始' % data[1])
    time.sleep(random.randint(1, 10))
    data = data[0].get()
    print(data)

if __name__ == '__main__':
    q = Queue()
    p_size = ThreadPoolExecutor(max_workers=5)
    for i in range(10):
        res = p_size.submit(func, q)
        res.add_done_callback(write_to_excel)
        print('用来感知回调是不是异步的: 答案,线程池的回调是异步的')
        """
        瞬间将10个任务丢进去，然后再shutdown那里停下来，等待所有任务执行完毕，包括回调，所有回调完成以后，执行 打印"结束"
        """
    p_size.shutdown()
    print('结束')
```



# 协程

https://www.cnblogs.com/Eva-J/articles/8324673.html

#### 协程概念

```python
# 本质上是一个线程
# 能够在多个任务之间切换来节省一些I/O时间
# 协程中任务之间的切换也消耗时间，但是开销要远远小于进程线程之间的切换
```



#### 协程工作一般的使用场景

```
1. 爬虫  （网络IO）
2. socket
好像就这样了
```



```python
def consumer():
    while True:
        x = yield
        print('处理一个数据: ', x)

def producer(f):
    temp = f()
    next(temp)
    for i in range(10):
        print('生产一个数据: ', i)
        temp.send(i)

producer(consumer)
```



#### 生成器

```
f = func()
next(f)
next(f)
send(100)

next(f)	---- 先执行next()，next启动生成器(f)并监听等待接收yield返回数据,f执行到yield，执行yield,yield有发送和监听功能,yield返回值给next,并监听send,第二次执行next()，next启动生产器并监听等待接收yield返回数据，继续执行后面的代码，当再次执行yield，yield返回数据给正在监听的next,并监听send

send(100)	----  执行send,发送数据给正在监听的yield，并监听yield的返回，并继续执行，再次执行到yield，执行yield，yield返回数据数据给正在监听的send后，yield继续监听


next() 是为了启动生成器
🔺next每次都会让生成器执行到yeild发完消息就结束，下一次next一样，使生成器执行到yield发完消息就结束，send（）每次都会让生成器执行的yield执行一次收发消息才。。单独使用send的时候，必须使用next激活生成器，而且yield发消息在收消息之前
🔺也可以这样理解，next每一次必须收一次消息， send()每次都要发一次消息。如果有循环，next执行完一次收，代码走到下一次收之前停止，send执行一次发以后，代码走到下一次发之前停止，这中间有一环，收在发之前，所以send也可以收
a = f.send()
```



#### greenlet 和 gevent 包



greenlet只是为了引入gevent这个概念，gevent才是为了来实现协程的

```python
def eat(name):
    print('%s eat 1' %name)
    g2.switch('egon')
    print('%s eat 2' %name)
    g2.switch()
def play(name):
    print('%s play 1' %name)
    g1.switch('alex')
    print('%s play 2' %name)
    
g1=greenlet(eat)
g2=greenlet(play)
g1.switch('Alex') #可以在第一次switch时传入参数，以后都不需要,他也只接收第一次的参数，后面再次穿参数将无效

```



#### 工作中的使用（数量）

进程数量 - 线程数量 - 协程数量

```
在工作中我们采用  进程 + 线程 + 协程 的方式来最大限度的提高代码的并发效果

如果你是4C的CUP 	 最多启动5个进程(4c+1)	 一个进程中启动20个线程(4c*5)   一个线程里面500个协程
4C机器一般执行的最大并发 = (4+1)*(4*5)*(500) = 50000

我的电脑是8C的 = (8+1)*(8*5)*500 = 18000
```



#### 协程 --- gevent （socket 和 爬虫）

```python
"""
Gevent 是一个第三方库，可以轻松通过gevent实现并发同步或异步编程，在gevent中用到的主要模式是Greenlet, 它是以C扩展模块形式接入Python的轻量级协程。 Greenlet全部运行在主程序操作系统进程的内部，但它们被协作式地调度。
"""


g1=gevent.spawn(func,1,,2,3,x=4,y=5)创建一个协程对象g1，spawn括号内第一个参数是函数名，如eat，后面可以有多个参数，可以是位置实参或关键字实参，都是传给函数eat的

g2=gevent.spawn(func2) #申明一个协程，跟p = process(target=func)一样，不会去执行

g1.join() #执行协程并等待g1结束

g2.join() #执行协程并等待g2结束

#或者上述两步合作一步：gevent.joinall([g1,g2])

g1.value#拿到func1的返回值

用法介绍
```





```python
import gevent
def eat(name):
    print('%s eat 1' %name)
    gevent.sleep(2)
    print('%s eat 2' %name)

def play(name):
    print('%s play 1' %name)
    gevent.sleep(1)
    print('%s play 2' %name)


g1=gevent.spawn(eat,'egon')
g2=gevent.spawn(play,name='egon')
g1.join()
g2.join()
#或者gevent.joinall([g1,g2])
print('主')
例：遇到io主动切换
上例gevent.sleep(2)模拟的是gevent可以识别的io阻塞,而time.sleep(2)或其他的阻塞,gevent是不能直接识别的需要用下面一行代码,打补丁,就可以识别了，看看下面的monkey.patch_all()
```



**🔺高级例子：gevent的Semaphore的wait方法**

> - ​	wait()监听一个计数,我们假设是count,count初始值为0,每次acquire()的时候count减1. 每次release()的时候,count加1, 当count不等于0的时候wait()阻塞,当count等于0的时候wait()结束阻塞

```python
import gevent
from gevent._semaphore import Semaphore
# from gevent import monkey;monkey.patch_all()

def func(num, sem):
    print(num)
    sem.acquire()
    print(num, '唱歌')
    gevent.sleep(10)
    print(num, '结束')
    sem.release()

def on_wait(sem):
    count = 1
    while True:
        print(count, '进入sem')
        sem.wait()   #🔺 当信号量为零的时候结束阻塞
        print(count, '上报:现在没有任务在执行')
        count += 1
if __name__ == '__main__':
    sem = Semaphore(4)
    p_list = []
    for i in range(10):
        p = gevent.spawn(func, i, sem)
        p_list.append(p)
    p = gevent.spawn(on_wait, sem)
    p_list.append(p)
    [j.join() for j in p_list]
```



#### from gevent import monkey;monkey.patch_all()

```python
from gevent import monkey;monkey.patch_all()
"""
monkey.patch_all() 会把它后面的所有import的模块当中的阻塞操作打成一个包，这时候它就能认识time模块的阻塞了
"""
import gevent
import time
def eat():
    print('eat food 1')
    time.sleep(2)
    print('eat food 2')

def play():
    print('play 1')
    time.sleep(1)
    print('play 2')

g1=gevent.spawn(eat)
g2=gevent.spawn(play)
gevent.joinall([g1,g2])
print('主')
```



```python
from gevent import monkey;monkey.patch_all()
import requests
import gevent

def get_url(url):
    res = requests.get(url)
    data = res.content
    return len(data)


url_list = ['http://www.baidu.com', 'http://www.sogou.com', 'http://www.taobao.com', 'http://www.hao123.com']
g_list = []
for i in url_list:
    g = gevent.spawn(get_url, i)
    g_list.append(g)
gevent.joinall(g_list)
for j in g_list:
    print(j.value)
```





# I/O多路复用 

https://www.cnblogs.com/Eva-J/articles/8324837.html

#### 阻塞IO （blocking IO）



###### 针对socket， accept 和 recv 都是读操作





#### 非阻塞IO

![](D:\笔记\python\非阻塞IO.png)





###### 实现socke

```python
"""
服务端
"""

import socket
from multiprocessing import Pool, Process

# p = Pool(5)
HOST = '127.0.0.1'
PORT = 21567
BUFSIZ = 1024
ADDR = (HOST, PORT)
tcpSerSock = socket.socket()
tcpSerSock.bind(ADDR)
tcpSerSock.setblocking(False)
tcpSerSock.listen(5)
add_conn = []
del_conn = []
while True:
    try:
        tcpCliSock, addr = tcpSerSock.accept()
        print('connection from ', addr)
        add_conn.append(tcpCliSock)
    except BlockingIOError:
        try:
            for sub_conn in add_conn:
                data = sub_conn.recv(BUFSIZ)            #没有收到消息这里会报BlockingIOError, 如果对面的socket断开以后，它会一直收到b''消息
                if data == b'': #收到b''说明sockt断开了
                    del_conn.append(sub_conn)
                    continue
                print(data.decode('utf8'))
        except BlockingIOError:
            for sub_conn in del_conn:
                add_conn.remove(sub_conn)
            del_conn.clear()
            
            
            
 """
 客户端
 """
import socket
import time
HOST = '127.0.0.1'
PORT = 21567
BUFSIZ = 1024
ADDR = (HOST, PORT)
sk = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
sk.connect(ADDR)
while True:
    # print(sk.recv(1024))
    msg = input('>>>').encode('utf8')
    sk.send(msg)
    
    
这里有一个致命的问题
但我启动client1 ， 在启动client2
由于for sub_conn in add_conn:
    每次都是从第一个conn连接开始接收数据，如果第一个client1不发送数据一直在input等着，那么上面的循环后面的连接全部都接收不到数据，这就很操了，还有其他性能上的问题，如下
```



#### 非阻塞IO模型绝不被推荐

我们不能否定则其优点：能够在等待任务完成的时间里干其他活了（包括提交其他任务，也就是 “后台” 可以有多个任务在“”同时“”执行）。

  但是也难掩其缺点：

```
#1. 循环调用recv()将大幅度推高CPU占用率；这也是我们在代码中留一句time.sleep(2)的原因,否则在低配主机下极容易出现卡机情况
#2. 任务完成的响应延迟增大了，因为每过一段时间才去轮询一次read操作，而任务可能在两次轮询之间的任意时间完成。这会导致整体数据吞吐量的降低。
3. while True 这玩意 非常占用CPU
```

  ***\*此外，在这个方案中recv()更多的是起到检测“操作是否完成”的作用，实际操作系统提供了更为高效的检测“操作是否完成“作用的接口，例如select()多路复用模式，可以一次检测多个连接是否活跃。\****



#### 🔺IO多路复用

![](D:\笔记\python\IO多路复用模型.png)

这个IO多路复用机制是操作系统提供的，不是我们代码级别的，操作系统级别的IO多路复用，上面是socket模型的IO多路复用模型， 在window上有一个专门提供IO多路复用的机制，叫select， 我们要从python代码去控制这个机制的话，我们就要使用python的select模块 



###### 其他

```python
一、I/O多路复用

I/O的含义：在计算机领域常说的IO包括磁盘IO和网络IO，我们所说的IO复用主要是指网络IO；在Linux中一切皆文件，因此网络IO也经常用文件描述符FD来表示。

复用的含义：在通信领域中为了充分利用网络连接的物理介质，往往在同一条网络链路上采用时分复用或频分复用的技术，使其在同一链路上传输多路信号，即公用某个“介质”来尽可能多的做同一类(性质)的事，在网络场景中复用的“介质”就是任务处理线程，所以简单理解就是多个IO共用1个线程。

I/O复用的好处：在传统意义上，对于多线程并发的处理方式是，服务端监听客户端请求，也就是I/O流，每有一个I/O流进来，程序就创建一个线程处理这个I/O流，假设现在有一百万个I/O流进来，那就需要开启一百万个线程一一对应处理这些I/O流，这样CPU占有率很高，而且这些I/O流可能大部分时间只是连接着，没有实际的数据读写，这也造成系统资源的浪费，所以人们提出了I/O多路复用这个模型，一个线程，通过记录I/O流的状态来同时管理多个I/O，可以提高服务器的吞吐能力。

因此就可以利用一个函数(select和poll)来监听I/O所需的这些数据的状态，一旦I/O有数据可以进行读写了，进程(也可以说是线程)就来对这样的IO进行服务。

select，poll，epoll都是IO多路复用的机制，I/O多路复用就是通过一种机制，让单个进程可以监视多个文件描述符，一旦某个描述符就绪(一般是读就绪或者写就绪)，能够通知应用程序进行相应的读写操作。

🔺但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。

二、Python select

Python的select()方法直接调用操作系统的IO接口，它监控sockets,open files, and pipes(所有带fileno()方法的文件句柄)何时变成readable 和writeable, 或者通信错误，select()使得同时监控多个连接变的简单，并且这比写一个长循环来等待和监控多客户端连接要高效，因为select直接通过操作系统提供的C的网络接口进行操作，而不是通过Python的解释器。

select()方法接收并监控3个通信列表， 第一个是所有的输入的data，即外部发过来的数据，第2个是所有要发出去的data，第3个监控错误信息。需要创建2个包含输入和输出信息列表来传给select()，列表里是服务端和客户端socket对象。

程序的主循环，调用select()时会阻塞和等待直到新的连接和数据进来。

当你把inputs,outputs,exceptional(这里跟inputs共用)传给select()后，它返回3个新的list，我们上面将他们分别赋值为readable,writable,exceptional, 所有在readable list中的socket连接代表有数据可接收(recv)，所有在writable list中的存放着你可以对其进行发送(send)操作的socket连接，当连接通信出现error时会把error写到exceptional列表中。

```



#### IO多路复用实现socket

```python
# -------------------------------------server---------------------------------------------------
import select
import socket


sk = socket.socket()
sk.bind(('127.0.0.1', 21567))
sk.setblocking(False)
sk.listen(10)
read_list = [sk]

while True:
    r_list, w_list, x_list = select.select(read_list, [], [])
    print('******', r_list) #r_list返回的是一个被监听的到对象
    for i in r_list:
        if i is sk:             #因为sk是server端的唯一值, socket对象
            conn, addr = i.accept()
            read_list.append(conn)
        else:
            try:
                data = i.recv(1024)
                if not data:
                    """
                    提示：
                    1. 当socket断开以后，服务端会一直接收到b''数据
                    2. 当socket强制断开以后，会报错
                    
                    """
                    i.close()
                    read_list.remove(i)
                    continue
                print(data.decode('utf8'))
                i.send(b'goodbye')
            except Exception as e:
                i.close()
                read_list.remove(i)
                
# -------------------------------------client---------------------------------------------------

from concurrent.futures import ThreadPoolExecutor
import socket
import time

def func():
    HOST = '127.0.0.1'
    PORT = 21567
    BUFSIZ = 1024
    ADDR = (HOST, PORT)
    sk = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sk.connect(ADDR)
    while True:
        # print(sk.recv(1024))
        sk.send('hello'.encode('utf8'))
        data = sk.recv(BUFSIZ)
        print(data)
        time.sleep(1)
        sk.close()
        break

a = ThreadPoolExecutor(5)
p_list = []
for i in range(20):
    a.submit(func, )

a.shutdown(True)
print('结束')
```

